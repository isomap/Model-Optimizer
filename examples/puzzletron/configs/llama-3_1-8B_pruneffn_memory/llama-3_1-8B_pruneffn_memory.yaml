defaults:
  - Llama-3_1-8B
  - _self_

# Input Hugging Face model to compress
input_hf_model_path: /workspace/hf_models/meta-llama/Llama-3.1-8B-Instruct

# Dataset path for pruning and NAS scoring
dataset_path: /workspace/datasets/Nemotron-Post-Training-Dataset-v2

# Working directory for puzzletron outputs
puzzle_dir: /workspace/puzzle_dir

# MIP memory constraint (in MiB)
mip:
  human_constraints:
    target_memory: 78_000 # 78 GiB
  # Memory sweep configuration (optional)
  sweep:
    enabled: false
    memory_compression_rates: [0.5, 0.6, 0.7, 0.8, 0.9]
    output_csv: ${puzzle_dir}/mip_sweep_results.csv

# FFN intermediate sizes to search over (heterogeneous architecture)
pruning:
  intermediate_size_list: [3072, 5888, 8704, 11520] # teacher_intermediate_size is 14336
