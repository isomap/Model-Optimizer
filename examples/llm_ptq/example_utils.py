# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import copy
import glob
import os
import shutil
import sys
import warnings
from pathlib import Path
from typing import Any

import torch
import transformers
from accelerate import infer_auto_device_map, init_empty_weights
from accelerate.utils import get_max_memory
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoProcessor,
    AutoTokenizer,
    PreTrainedTokenizerBase,
    ProcessorMixin,
)

try:
    from huggingface_hub import snapshot_download
except ImportError:
    snapshot_download = None

import modelopt.torch.quantization as mtq
from modelopt.torch.utils.image_processor import BaseImageProcessor, MllamaImageProcessor

SPECULATIVE_MODEL_LIST = ["Eagle", "Medusa"]


def run_nemotron_vl_preview(
    full_model, tokenizer, input_ids, pyt_ckpt_path, stage_name, allow_fallback=False
):
    """Run text-only and VL preview generation for Nemotron VL models.

    Args:
        full_model: The full VL model
        tokenizer: The tokenizer
        input_ids: Input tensor for generation
        pyt_ckpt_path: Path to the model checkpoint
        stage_name: Description of the stage (e.g., "before quantization", "after quantization")
        allow_fallback: Whether to allow fallback to standard generate on failure

    Returns:
        Generated text response or None if generation failed
    """
    from vlm_utils import run_text_only_generation, run_vl_preview_generation

    print(f"Running text-only preview generation for Nemotron VL model ({stage_name})...")
    question = tokenizer.decode(input_ids[0], skip_special_tokens=True)
    generation_config = {
        "max_new_tokens": 100,
        "do_sample": False,
        "eos_token_id": tokenizer.eos_token_id,
    }

    # Try text-only generation
    text_response = run_text_only_generation(
        full_model, tokenizer, question, generation_config, pyt_ckpt_path
    )

    if text_response is not None:
        print(f"âœ… Text-only generation successful: {text_response[:100]}...")
        generated_ids = text_response
    elif allow_fallback:
        print("Text-only generation failed, falling back to standard generate...")
        generated_ids = full_model.generate(input_ids, max_new_tokens=100)
    else:
        generated_ids = None

    # Run additional VL test with images
    print(f"Running additional VL test with images ({stage_name})...")
    run_vl_preview_generation(full_model, tokenizer, pyt_ckpt_path, stage_name)

    return generated_ids


def _is_multimodal_config(config):
    """Check if a config indicates a multimodal model (config-only version of is_multimodal_model)."""
    return (
        hasattr(config, "vision_config")  # Standard vision config (e.g., Qwen2.5-VL)
        or getattr(config, "model_type", "") == "phi4mm"  # Phi-4 multimodal
        or hasattr(config, "vision_lora")  # Vision LoRA configurations
        or hasattr(config, "audio_processor")  # Audio processing capabilities
        or (
            hasattr(config, "embd_layer") and hasattr(config.embd_layer, "image_embd_layer")
        )  # Image embedding layers
    )


def is_nemotron_vl(model_or_config):
    """Check if model or config indicates a Nemotron VL model.

    Args:
        model_or_config: Either a model instance or a config object.

    Returns:
        bool: True if it's a Nemotron VL model, False otherwise.
    """
    # Try to get config from model, or use directly if it's a config
    if hasattr(model_or_config, "config"):
        config = model_or_config.config
        from modelopt.torch.export.model_utils import is_multimodal_model

        if not is_multimodal_model(model_or_config):
            return False
    else:
        config = model_or_config
        if not _is_multimodal_config(config):
            return False

    architectures = getattr(config, "architectures", [])
    return any("nemotron" in arch.lower() for arch in architectures)


def build_quant_cfg(
    qformat,
    kv_cache_qformat,
    awq_block_size,
    model_type,
    quant_cfg_choices,
    kv_quant_cfg_choices,
) -> dict[str, Any]:
    quant_cfg = {}
    assert qformat in quant_cfg_choices, (
        f"Unsupported quantization format: {qformat} with {kv_cache_qformat} KV cache"
    )

    quant_cfg = quant_cfg_choices[qformat]

    if "awq" in qformat:
        quant_cfg = copy.deepcopy(quant_cfg_choices[qformat])
        weight_quantizer = quant_cfg["quant_cfg"]["*weight_quantizer"]
        if isinstance(weight_quantizer, list):
            weight_quantizer = weight_quantizer[0]
        # If awq_block_size argument is provided, update weight_quantizer
        if awq_block_size:
            weight_quantizer["block_sizes"][-1] = awq_block_size

        # Coarser optimal scale search seems to resolve the overflow in TRT-LLM for some models
        if qformat == "w4a8_awq" and model_type in ["gemma", "mpt"]:
            quant_cfg["algorithm"] = {"method": "awq_lite", "alpha_step": 1}

    enable_quant_kv_cache = kv_cache_qformat != "none"
    print(f"{'Enable' if enable_quant_kv_cache else 'Disable'} KV cache quantization")

    # Check if any bmm_quantizer is in the quant_cfg. If so, we need to enable the bmm_quantizer.
    if enable_quant_kv_cache:
        quant_cfg = mtq.update_quant_cfg_with_kv_cache_quant(
            quant_cfg,
            getattr(mtq, kv_quant_cfg_choices[kv_cache_qformat])["quant_cfg"],
        )

    # Gemma 7B has accuracy regression using alpha 1. We set 0.5 instead.
    if model_type == "gemma" and "int8_sq" in qformat:
        quant_cfg["algorithm"] = {"method": "smoothquant", "alpha": 0.5}

    if model_type == "phi4mm":
        # Only quantize the language model
        quant_cfg["quant_cfg"]["*speech*"] = {"enable": False}
        quant_cfg["quant_cfg"]["*audio*"] = {"enable": False}
        quant_cfg["quant_cfg"]["*image*"] = {"enable": False}
        quant_cfg["quant_cfg"]["*vision*"] = {"enable": False}

    if model_type in ["qwen3moe", "qwen3next"] and qformat == "nvfp4":
        # Disable the attention projection layers to retain accuracy
        quant_cfg["quant_cfg"]["model*.*attn*in_proj*"] = {"enable": False}
        quant_cfg["quant_cfg"]["model*.*attn*q_proj*"] = {"enable": False}
        quant_cfg["quant_cfg"]["model*.*attn*k_proj*"] = {"enable": False}
        quant_cfg["quant_cfg"]["model*.*attn*v_proj*"] = {"enable": False}

    if model_type == "deepseek":
        # Disable MLA quantization for accuracy.
        quant_cfg["quant_cfg"]["*self_attn.q*"] = {"enable": False}
        quant_cfg["quant_cfg"]["*self_attn.kv*"] = {"enable": False}

    return quant_cfg


def is_speculative(hf_config):
    """Check if the model architecture is a speculative model."""
    return hf_config.architectures and any(
        name in hf_config.architectures[0] for name in SPECULATIVE_MODEL_LIST
    )


def get_tokenizer(ckpt_path, trust_remote_code=False, **kwargs) -> PreTrainedTokenizerBase:
    print(f"Initializing tokenizer from {ckpt_path}")

    if "vila" in ckpt_path.lower():
        ckpt_path += "/llm"

    tokenizer = AutoTokenizer.from_pretrained(
        ckpt_path, trust_remote_code=trust_remote_code, **kwargs
    )

    # can't set attribute 'pad_token' for "<unk>"
    # We skip this step for Nemo models
    if tokenizer.pad_token != "<unk>" or tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    assert tokenizer.pad_token is not None, f"Pad token for {ckpt_path} cannot be set!"

    return tokenizer


def get_processor(
    ckpt_path,
    model_type,
    device: torch.device = "auto",
    trust_remote_code=False,
    attn_implementation=None,
) -> BaseImageProcessor | ProcessorMixin | None:
    """
    Returns a :class:`modelopt.torch.utils.image_processor.MllamaImageProcessor` object.
    """
    model_kwargs = {"trust_remote_code": trust_remote_code}
    if attn_implementation is not None:
        model_kwargs["attn_implementation"] = attn_implementation

    if model_type == "whisper":
        processor = AutoProcessor.from_pretrained(
            ckpt_path,
            padding_side="left",
            **model_kwargs,
        )
        if processor.tokenizer.pad_token is None:
            processor.tokenizer.pad_token = processor.tokenizer.eos_token
        assert processor.tokenizer.pad_token is not None, (
            f"Pad token for {ckpt_path} cannot be set!"
        )

        return processor
    elif model_type == "mllama":
        processor = AutoProcessor.from_pretrained(
            ckpt_path,
            padding_side="left",
            **model_kwargs,
        )
        if processor.tokenizer.pad_token is None:
            processor.tokenizer.pad_token = processor.tokenizer.eos_token
        assert processor.tokenizer.pad_token is not None, (
            f"Pad token for {ckpt_path} cannot be set!"
        )

        return MllamaImageProcessor(processor, device)

    return None


def get_dtype(dtype):
    if dtype == "bf16":
        dtype = torch.bfloat16
    elif dtype == "fp16":
        dtype = torch.float16
    elif dtype == "fp32":
        dtype = torch.float32
    else:
        raise NotImplementedError(f"Unknown dtype {dtype}")

    return dtype


def _patch_compressed_linear_init():
    """Patch CompressedLinear to prevent transformers weight initialization errors.

    When loading pack-quantized models, CompressedLinear modules don't have a 'weight'
    attribute (they have weight_packed instead). Transformers tries to initialize
    missing weights which fails. This patch adds a dummy weight property.
    """
    try:
        from compressed_tensors.linear.compressed_linear import CompressedLinear
    except ImportError:
        return  # compressed_tensors not installed

    if hasattr(CompressedLinear, "_modelopt_init_patched"):
        return  # Already patched

    # Patch __getattr__ to return dummy for weight access
    original_getattr = getattr(CompressedLinear, "__getattr__", None)

    class DummyWeightData:
        """Dummy tensor data that accepts initialization calls like .normal_(), .zero_()."""

        def __getattr__(self, name):
            # Return self for any method call to allow chaining
            return lambda *args, **kwargs: self

    class DummyWeight:
        """Dummy weight with .data that accepts any initialization."""

        def __init__(self):
            self.data = DummyWeightData()

        def __getattr__(self, name):
            return lambda *args, **kwargs: self

    def patched_getattr(self, name):
        if name == "weight":
            # Check if real weight exists
            if "_parameters" in self.__dict__ and "weight" in self._parameters:
                return self._parameters["weight"]
            if "weight" in self.__dict__:
                return self.__dict__["weight"]
            # Return dummy weight for initialization purposes (don't store it)
            return DummyWeight()
        if original_getattr is not None:
            return original_getattr(self, name)
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")

    CompressedLinear.__getattr__ = patched_getattr
    CompressedLinear._modelopt_init_patched = True
    print("Patched CompressedLinear for transformers compatibility")


def _unpack_compressed_linear_weights(model, ckpt_path=None):
    """Unpack all CompressedLinear weights from INT4 to BF16.

    This decompresses the packed INT4 weights to BF16 format so they can be
    processed by Model-Optimizer's PTQ flow. This must be called after model
    loading but before quantization/calibration.

    Args:
        model: The loaded model with CompressedLinear modules
        ckpt_path: Path to the checkpoint to reload int32 weights from safetensors
    """
    try:
        from compressed_tensors.linear.compressed_linear import CompressedLinear
    except ImportError:
        return  # compressed_tensors not installed, nothing to unpack

    # Get checkpoint path from model config if not provided
    if ckpt_path is None:
        if hasattr(model, "config") and hasattr(model.config, "_name_or_path"):
            ckpt_path = model.config._name_or_path

    # Load original int32 weights from safetensors
    original_weights = {}
    if ckpt_path:
        import json
        import os

        from safetensors import safe_open

        # Find safetensors files
        index_path = os.path.join(ckpt_path, "model.safetensors.index.json")
        single_path = os.path.join(ckpt_path, "model.safetensors")

        safetensor_files = []
        if os.path.exists(index_path):
            with open(index_path) as f:
                index = json.load(f)
            safetensor_files = list(set(index.get("weight_map", {}).values()))
            safetensor_files = [os.path.join(ckpt_path, f) for f in safetensor_files]
        elif os.path.exists(single_path):
            safetensor_files = [single_path]

        # Load int32 weights (weight_packed, weight_shape)
        for sf_path in safetensor_files:
            with safe_open(sf_path, framework="pt") as f:
                for key in f.keys():
                    if "weight_packed" in key or "weight_shape" in key:
                        original_weights[key] = f.get_tensor(key)

        print(f"Loaded {len(original_weights)} packed weight tensors from safetensors")

    def _lookup_original(key: str):
        if not original_weights:
            return None
        if key in original_weights:
            return original_weights[key]
        # Fallback: match by suffix when prefixes differ
        matches = [k for k in original_weights if k.endswith(key)]
        if len(matches) == 1:
            return original_weights[matches[0]]
        return None

    unpacked_count = 0
    for name, module in model.named_modules():
        if isinstance(module, CompressedLinear) and hasattr(module, "weight_packed"):
            with torch.no_grad():
                # Build compressed_data dict
                compressed_data = {}

                # Use original int32 weights from safetensors if available
                packed_key = f"{name}.weight_packed"
                shape_key = f"{name}.weight_shape"

                packed_tensor = _lookup_original(packed_key)
                if packed_tensor is None:
                    packed_tensor = module.weight_packed
                elif isinstance(packed_tensor, torch.Tensor):
                    packed_tensor = packed_tensor.to(module.weight_packed.device)
                compressed_data["weight_packed"] = packed_tensor

                if isinstance(compressed_data["weight_packed"], torch.Tensor):
                    if compressed_data["weight_packed"].dtype != torch.int32:
                        # Some modules (e.g., vision tower) are not pack-quantized.
                        # Skip decompression if packed weights are not int32.
                        continue

                shape_tensor = _lookup_original(shape_key)
                if shape_tensor is not None:
                    if isinstance(shape_tensor, torch.Tensor):
                        shape_tensor = shape_tensor.to(module.weight_packed.device)
                    compressed_data["weight_shape"] = [int(x) for x in shape_tensor.tolist()]
                elif hasattr(module, "weight_shape"):
                    ws = module.weight_shape
                    if isinstance(ws, torch.Tensor):
                        compressed_data["weight_shape"] = [int(x) for x in ws.tolist()]
                    elif isinstance(ws, (list, tuple)):
                        compressed_data["weight_shape"] = [int(x) for x in ws]
                    else:
                        compressed_data["weight_shape"] = ws

                # Get weight_scale (this one is float, so conversion is OK)
                if hasattr(module, "weight_scale"):
                    compressed_data["weight_scale"] = module.weight_scale.to(
                        module.weight_packed.device
                    )

                if hasattr(module, "weight_zero_point"):
                    compressed_data["weight_zero_point"] = module.weight_zero_point.to(
                        module.weight_packed.device
                    )

                # Get quantization args
                quant_args = None
                if hasattr(module, "quantization_scheme") and module.quantization_scheme:
                    if hasattr(module.quantization_scheme, "weights"):
                        quant_args = module.quantization_scheme.weights

                # Decompress on CPU to avoid GPU OOM, then move to target device.
                decompress_device = torch.device("cpu")
                if isinstance(compressed_data["weight_packed"], torch.Tensor):
                    compressed_data["weight_packed"] = compressed_data["weight_packed"].to(
                        decompress_device
                    )
                if "weight_scale" in compressed_data and isinstance(
                    compressed_data["weight_scale"], torch.Tensor
                ):
                    compressed_data["weight_scale"] = compressed_data["weight_scale"].to(
                        decompress_device
                    )
                if "weight_zero_point" in compressed_data and isinstance(
                    compressed_data["weight_zero_point"], torch.Tensor
                ):
                    compressed_data["weight_zero_point"] = compressed_data["weight_zero_point"].to(
                        decompress_device
                    )

                decompressed_weight = module.compressor.decompress_weight(
                    compressed_data=compressed_data,
                    quantization_args=quant_args,
                ).to(module.weight_packed.device, non_blocking=True)

                # Register as parameter (avoid register_parameter to bypass __getattr__ hook)
                module._parameters.pop("weight", None)
                module._buffers.pop("weight", None)
                if "weight" in module.__dict__:
                    del module.__dict__["weight"]
                param = torch.nn.Parameter(decompressed_weight, requires_grad=False)
                module._parameters["weight"] = param
                module.__dict__["weight"] = param

                # Clean up
                if hasattr(module, "weight_packed"):
                    delattr(module, "weight_packed")
                if hasattr(module, "weight_scale"):
                    delattr(module, "weight_scale")
                if hasattr(module, "weight_shape"):
                    if "weight_shape" in module._parameters:
                        del module._parameters["weight_shape"]
                    elif hasattr(module, "weight_shape"):
                        delattr(module, "weight_shape")

                from compressed_tensors.quantization import QuantizationStatus

                module.quantization_status = QuantizationStatus.FROZEN
            unpacked_count += 1

    if unpacked_count > 0:
        print(f"Unpacked {unpacked_count} CompressedLinear modules from INT4 to BF16")


def get_model(
    ckpt_path,
    device="cuda",
    gpu_mem_percentage=0.8,
    trust_remote_code=False,
    use_seq_device_map=False,
    attn_implementation=None,
):
    print(f"Initializing model from {ckpt_path}")

    # Note: CompressedLinear weights will be unpacked after model loading

    device_map = "auto"
    if device == "cpu":
        device_map = "cpu"

    # Add VILA to sys.path before loading config if needed
    if "vila" in ckpt_path.lower():
        vila_path = os.path.join(ckpt_path, "..", "VILA")
        if vila_path not in sys.path:
            sys.path.append(vila_path)
        from llava.model import LlavaLlamaConfig, LlavaLlamaModel  # noqa: F401

    # Prepare config kwargs for loading
    config_kwargs = {"trust_remote_code": trust_remote_code} if trust_remote_code else {}

    # Load config once and handle VL model detection
    try:
        hf_config = AutoConfig.from_pretrained(ckpt_path, **config_kwargs)
        if is_nemotron_vl(hf_config):
            print(
                "Detected Nemotron VL model from config. "
                "Disabling automatic device mapping for compatibility."
            )
            device_map = None
    except Exception as e:
        print(f"Error: Could not load config from {ckpt_path}: {e}")
        raise RuntimeError(f"Failed to load model configuration from {ckpt_path}") from e
    if attn_implementation is not None:
        config_kwargs["attn_implementation"] = attn_implementation

    # Note: Forcibly converting the model precision between bf16 and fp16 may introduce accuracy drop
    model_kwargs = config_kwargs.copy()
    # Don't set torch_dtype for VILA models as they handle it explicitly in their builder
    if "vila" not in ckpt_path.lower():
        model_kwargs.setdefault("torch_dtype", "auto")

    if "vila" in ckpt_path.lower():
        from transformers import AutoModel

        hf_vila = AutoModel.from_pretrained(
            ckpt_path,
            device_map=device_map,
            **model_kwargs,
        )
        model = hf_vila.llm
    else:
        if use_seq_device_map:
            device_map = "sequential"
            # If we use sequential, set max_memory limit to ensure that the model does not occupy the full GPU
            max_memory = get_max_memory()
            max_memory = {key: value * gpu_mem_percentage for key, value in max_memory.items()}
            model_kwargs["max_memory"] = max_memory

        if hf_config.model_type == "bart":
            # device_map "auto" and "cuda" triggers error regarding meta tensor from safetensors
            device_map = None

        # Helper function to check if model has pack-quantized config
        def has_pack_quantized_config(config):
            # Check top-level quantization_config
            if hasattr(config, "quantization_config"):
                if config.quantization_config.get("format", None) == "pack-quantized":
                    return True
            # Check nested text_config.quantization_config (for multi-modal models like kimi k2.5)
            if hasattr(config, "text_config") and hasattr(
                config.text_config, "quantization_config"
            ):
                if config.text_config.quantization_config.get("format", None) == "pack-quantized":
                    return True
            return False

        if is_speculative(hf_config):
            model = AutoModelForCausalLM.from_pretrained(
                ckpt_path,
                device_map=device_map,
                **model_kwargs,
            )
        elif has_pack_quantized_config(hf_config):
            # Patch CompressedLinear before loading to handle missing weight attribute
            _patch_compressed_linear_init()
            # Pass torch_dtype="auto" to preserve original dtypes from safetensors
            # This prevents int32 packed weights from being converted to float
            model = AutoModelForCausalLM.from_pretrained(
                ckpt_path,
                device_map="auto",
                trust_remote_code=trust_remote_code,
                torch_dtype="auto",
            )
        else:
            architecture = hf_config.architectures[0]

            if not hasattr(transformers, architecture) or "Deepseek" in architecture:
                if not hasattr(transformers, architecture):
                    warnings.warn(
                        f"Architecture {architecture} not found in transformers: {transformers.__version__}. "
                        "Falling back to AutoModelForCausalLM."
                    )
                assert trust_remote_code, (
                    "Please set trust_remote_code to True if you want to use this architecture"
                )

                auto_model_module = AutoModelForCausalLM
                from_config = auto_model_module.from_config
            else:
                auto_model_module = getattr(transformers, architecture)
                from_config = auto_model_module._from_config

            with init_empty_weights():
                # When computing the device_map, assuming bfloat16 precision by default,
                # unless specified by the hf_config.
                torch_dtype = getattr(hf_config, "torch_dtype", torch.bfloat16)
                model_kwargs2 = model_kwargs.copy()
                if auto_model_module != AutoModelForCausalLM:
                    model_kwargs2.pop("trust_remote_code", None)
                model_kwargs2["torch_dtype"] = torch_dtype
                model_kwargs2.pop("max_memory", None)
                model = from_config(hf_config, **model_kwargs2)

            max_memory = get_max_memory()
            inferred_device_map = infer_auto_device_map(model, max_memory=max_memory)

            on_cpu = "cpu" in inferred_device_map.values()

            if on_cpu:
                for _device in max_memory:
                    if isinstance(_device, int):
                        max_memory[_device] *= gpu_mem_percentage

                print(
                    "Model does not fit to the GPU mem. "
                    f"We apply the following memory limit for calibration: \n{max_memory}\n"
                    "If you hit GPU OOM issue, please adjust `gpu_mem_percentage` or "
                    "reduce the calibration `batch_size` manually."
                )
                model_kwargs["max_memory"] = max_memory

            model = auto_model_module.from_pretrained(
                ckpt_path,
                device_map=device_map,
                **model_kwargs,
            )
    model.eval()

    # Unpack CompressedLinear weights (INT4 -> BF16) if present
    # This is needed for models with compressed-tensors quantization (e.g., kimi k2.5)
    _unpack_compressed_linear_weights(model, ckpt_path)

    # If device_map was disabled (None), manually move model to target device
    if device_map is None and device != "cpu":
        print(f"Moving model to {device} device...")
        model = model.to(device)

    if device == "cuda" and not is_model_on_gpu(model):
        print("Warning: Some parameters are not on a GPU. Calibration can be slow or hit OOM")

    return model


def is_model_on_gpu(model) -> bool:
    """Returns if the model is fully loaded on GPUs."""
    return all("cuda" in str(param.device) for param in model.parameters())


def is_enc_dec(model_type) -> bool:
    """Return if the model is a encoder-decoder model."""
    return model_type in ["t5", "bart", "whisper"]


def _resolve_model_path(model_name_or_path: str, trust_remote_code: bool = False) -> str:
    """Resolve a model name or path to a local directory path.

    If the input is already a local directory, returns it as-is.
    If the input is a HuggingFace model ID, attempts to resolve it to the local cache path.

    Args:
        model_name_or_path: Either a local directory path or HuggingFace model ID
        trust_remote_code: Whether to trust remote code when loading the model

    Returns:
        Local directory path to the model files
    """
    # If it's already a local directory, return as-is
    if os.path.isdir(model_name_or_path):
        return model_name_or_path

    # Try to resolve HuggingFace model ID to local cache path
    try:
        # First try to load the config to trigger caching
        config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=trust_remote_code)

        # The config object should have the local path information
        # Try different ways to get the cached path
        if hasattr(config, "_name_or_path") and os.path.isdir(config._name_or_path):
            return config._name_or_path

        # Alternative: use snapshot_download if available
        if snapshot_download is not None:
            try:
                local_path = snapshot_download(
                    repo_id=model_name_or_path,
                    allow_patterns=["*.py", "*.json"],  # Only download Python files and config
                )
                return local_path
            except Exception as e:
                print(f"Warning: Could not download model files using snapshot_download: {e}")

        # Fallback: try to find in HuggingFace cache
        from transformers.utils import TRANSFORMERS_CACHE

        # Look for the model in the cache directory
        cache_pattern = os.path.join(TRANSFORMERS_CACHE, "models--*")
        cache_dirs = glob.glob(cache_pattern)

        # Convert model name to cache directory format
        model_cache_name = model_name_or_path.replace("/", "--")
        for cache_dir in cache_dirs:
            if model_cache_name in cache_dir:
                # Look for the snapshots directory
                snapshots_dir = os.path.join(cache_dir, "snapshots")
                if os.path.exists(snapshots_dir):
                    # Get the latest snapshot
                    snapshot_dirs = [
                        d
                        for d in os.listdir(snapshots_dir)
                        if os.path.isdir(os.path.join(snapshots_dir, d))
                    ]
                    if snapshot_dirs:
                        latest_snapshot = max(snapshot_dirs)  # Use lexicographically latest
                        snapshot_path = os.path.join(snapshots_dir, latest_snapshot)
                        return snapshot_path

    except Exception as e:
        print(f"Warning: Could not resolve model path for {model_name_or_path}: {e}")

    # If all else fails, return the original path
    # This will cause the copy function to skip with a warning
    return model_name_or_path


def copy_custom_model_files(source_path: str, export_path: str, trust_remote_code: bool = False):
    """Copy custom model files (configuration_*.py, modeling_*.py, *.json, etc.) from source to export directory.

    This function copies custom Python files and JSON configuration files that are needed for
    models with custom code. It excludes config.json and model.safetensors.index.json as these
    are typically handled separately by the model export process.

    Args:
        source_path: Path to the original model directory or HuggingFace model ID
        export_path: Path to the exported model directory
        trust_remote_code: Whether trust_remote_code was used (only copy files if True)
    """
    if not trust_remote_code:
        return

    # Resolve the source path (handles both local paths and HF model IDs)
    resolved_source_path = _resolve_model_path(source_path, trust_remote_code)

    source_dir = Path(resolved_source_path)
    export_dir = Path(export_path)

    if not source_dir.exists():
        if resolved_source_path != source_path:
            print(
                f"Warning: Could not find local cache for HuggingFace model '{source_path}' "
                f"(resolved to '{resolved_source_path}')"
            )
        else:
            print(f"Warning: Source directory '{source_path}' does not exist")
        return

    if not export_dir.exists():
        print(f"Warning: Export directory {export_path} does not exist")
        return

    # Common patterns for custom model files that need to be copied
    custom_file_patterns = [
        "configuration_*.py",
        "modeling*.py",
        "tokenization_*.py",
        "processing_*.py",
        "image_processing*.py",
        "feature_extraction_*.py",
        "*.json",
    ]

    copied_files = []
    for pattern in custom_file_patterns:
        for file_path in source_dir.glob(pattern):
            if file_path.is_file():
                # Skip config.json and model.safetensors.index.json as they're handled separately
                if file_path.name in ["config.json", "model.safetensors.index.json"]:
                    continue
                dest_path = export_dir / file_path.name
                try:
                    shutil.copy2(file_path, dest_path)
                    copied_files.append(file_path.name)
                    print(f"Copied custom model file: {file_path.name}")
                except Exception as e:
                    print(f"Warning: Failed to copy {file_path.name}: {e}")

    if copied_files:
        print(f"Successfully copied {len(copied_files)} custom model files to {export_path}")
    else:
        print("No custom model files found to copy")
